{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZ46dK5XI6_R"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "from pathlib import Path\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/gdrive\", force_remount = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7bMo97ym6qJQ"
   },
   "outputs": [],
   "source": [
    "# #path = \" Enter the folder path of Unzipped UCI Daphnet Dataset \"\n",
    "# path= \"/Users/vanshika/Downloads/dataset_fog_release/dataset_fog_release/dataset\"\n",
    "# os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9k_QktDOd14e"
   },
   "source": [
    "# **To Label the Data with 3rd class (Pre-FOG)**\n",
    "Before the occurence of every FOG event, the previous w*f_s timesteps are labelled as a third class 'preFOG' which can be trained in order to predict FOG before it's onset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0yg93EeI6_r"
   },
   "outputs": [],
   "source": [
    "def label_prefog(dataset,window_length = 1):\n",
    "    dataset.drop(index = list(dataset[dataset['Action'] == 0].index),inplace=True)\n",
    "    window_length = 64*window_length\n",
    "\n",
    "    fog_index=[]\n",
    "    for i in dataset.index: \n",
    "        if dataset.loc[i,'Action'] == 2:\n",
    "            fog_index.append(i)\n",
    "    #fog_index\n",
    "\n",
    "    start_indices=[]\n",
    "    for i in fog_index:\n",
    "        if (dataset.loc[i-1,'Action']!=dataset.loc[i,'Action']):\n",
    "            start_indices.append(i)\n",
    "\n",
    "    prefog=[]\n",
    "    for start in start_indices:\n",
    "        prefog_start = [x for x in range(start-window_length,start)]\n",
    "        prefog.append(prefog_start)\n",
    "\n",
    "    prefog = [item for sublist in prefog for item in sublist]\n",
    "\n",
    "    for i in prefog:\n",
    "        dataset.loc[i,'Action'] = 3\n",
    "    dataset['Action'] = dataset['Action'] - 1\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPi05GU1I6_7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R01.txt  is read\tAdding S06R01.txt to dataset\tS06R01.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R02.txt  is read\t\n",
      "S07\n",
      "S07R01.txt  is read\tAdding S07R01.txt to dataset\tS07R01.txt is labelled\n",
      "\n",
      "S07\n",
      "S07R02.txt  is read\tAdding S07R02.txt to dataset\tS07R02.txt is labelled\n",
      "\n",
      "S08\n",
      "S08R01.txt  is read\tAdding S08R01.txt to dataset\tS08R01.txt is labelled\n",
      "\n",
      "S09\n",
      "S09R01.txt  is read\tAdding S09R01.txt to dataset\tS09R01.txt is labelled\n",
      "\n",
      "S10\n",
      "S10R01.txt  is read\t\n",
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R01.txt  is read\tAdding S06R01.txt to dataset\tS06R01.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R02.txt  is read\t\n",
      "S07\n",
      "S07R01.txt  is read\tAdding S07R01.txt to dataset\tS07R01.txt is labelled\n",
      "\n",
      "S07\n",
      "S07R02.txt  is read\tAdding S07R02.txt to dataset\tS07R02.txt is labelled\n",
      "\n",
      "S08\n",
      "S08R01.txt  is read\tAdding S08R01.txt to dataset\tS08R01.txt is labelled\n",
      "\n",
      "S09\n",
      "S09R01.txt  is read\tAdding S09R01.txt to dataset\tS09R01.txt is labelled\n",
      "\n",
      "S10\n",
      "S10R01.txt  is read\t\n",
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R01.txt  is read\tAdding S06R01.txt to dataset\tS06R01.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R02.txt  is read\t\n",
      "S07\n",
      "S07R01.txt  is read\tAdding S07R01.txt to dataset\tS07R01.txt is labelled\n",
      "\n",
      "S07\n",
      "S07R02.txt  is read\tAdding S07R02.txt to dataset\tS07R02.txt is labelled\n",
      "\n",
      "S08\n",
      "S08R01.txt  is read\tAdding S08R01.txt to dataset\tS08R01.txt is labelled\n",
      "\n",
      "S09\n",
      "S09R01.txt  is read\tAdding S09R01.txt to dataset\tS09R01.txt is labelled\n",
      "\n",
      "S10\n",
      "S10R01.txt  is read\t\n",
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R01.txt  is read\tAdding S06R01.txt to dataset\tS06R01.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R02.txt  is read\t\n",
      "S07\n",
      "S07R01.txt  is read\tAdding S07R01.txt to dataset\tS07R01.txt is labelled\n",
      "\n",
      "S07\n",
      "S07R02.txt  is read\tAdding S07R02.txt to dataset\tS07R02.txt is labelled\n",
      "\n",
      "S08\n",
      "S08R01.txt  is read\tAdding S08R01.txt to dataset\tS08R01.txt is labelled\n",
      "\n",
      "S09\n",
      "S09R01.txt  is read\tAdding S09R01.txt to dataset\tS09R01.txt is labelled\n",
      "\n",
      "S10\n",
      "S10R01.txt  is read\t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3298672, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"/Users/vanshika/Downloads/dataset_fog_release/dataset_fog_release/dataset\"\n",
    "#\" Enter the folder path of Unzipped UCI Daphnet Dataset \"\n",
    "\n",
    "people = []\n",
    "dataset=pd.DataFrame()\n",
    "for person in os.listdir(data_path):\n",
    "    if '.txt' in person: \n",
    "        people.append(person)\n",
    "        \n",
    "for window_length in range(1,5):\n",
    "    for person in people: \n",
    "        name = person.split('R')[0]\n",
    "        print (name)\n",
    "        file = data_path+\"/\"+person\n",
    "        temp = pd.read_csv(file,delimiter= \" \", header = None)\n",
    "        print (person,' is read',end = '\\t')\n",
    "\n",
    "        if 2 in temp[max(temp.columns)].unique():\n",
    "            print ('Adding {} to dataset'.format(person),end = '\\t')            \n",
    "            temp.columns = ['time','A_F','A_V','A_L','L_F','L_V','L_L','T_F','T_V','T_L','Action']   \n",
    "            temp = label_prefog(temp,window_length).reset_index(drop=True)\n",
    "\n",
    "            temp['name'] = name\n",
    "            \n",
    "            print ('{} is labelled'.format(person))\n",
    "            dataset = pd.concat([dataset,temp],axis = 0)\n",
    "\n",
    "        print ('')\n",
    "    dataset.reset_index(drop =True,inplace=True) \n",
    "    dataset.to_csv(\"/Users/vanshika/Downloads/dataset_fog_release/dataset_fog_release/dataset/data.csv\",index = None)\n",
    "\n",
    "\n",
    "display(dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>A_F</th>\n",
       "      <th>A_V</th>\n",
       "      <th>A_L</th>\n",
       "      <th>L_F</th>\n",
       "      <th>L_V</th>\n",
       "      <th>L_L</th>\n",
       "      <th>T_F</th>\n",
       "      <th>T_V</th>\n",
       "      <th>T_L</th>\n",
       "      <th>Action</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>750000</td>\n",
       "      <td>-30</td>\n",
       "      <td>990</td>\n",
       "      <td>326</td>\n",
       "      <td>-45</td>\n",
       "      <td>972</td>\n",
       "      <td>181</td>\n",
       "      <td>-38</td>\n",
       "      <td>1000</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>750015</td>\n",
       "      <td>-30</td>\n",
       "      <td>1000</td>\n",
       "      <td>356</td>\n",
       "      <td>-18</td>\n",
       "      <td>981</td>\n",
       "      <td>212</td>\n",
       "      <td>-48</td>\n",
       "      <td>1028</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>750031</td>\n",
       "      <td>-20</td>\n",
       "      <td>990</td>\n",
       "      <td>336</td>\n",
       "      <td>18</td>\n",
       "      <td>981</td>\n",
       "      <td>222</td>\n",
       "      <td>-38</td>\n",
       "      <td>1038</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>750046</td>\n",
       "      <td>-20</td>\n",
       "      <td>1000</td>\n",
       "      <td>316</td>\n",
       "      <td>36</td>\n",
       "      <td>990</td>\n",
       "      <td>222</td>\n",
       "      <td>-19</td>\n",
       "      <td>1038</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>750062</td>\n",
       "      <td>0</td>\n",
       "      <td>990</td>\n",
       "      <td>316</td>\n",
       "      <td>36</td>\n",
       "      <td>990</td>\n",
       "      <td>212</td>\n",
       "      <td>-29</td>\n",
       "      <td>1038</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     time  A_F   A_V  A_L  L_F  L_V  L_L  T_F   T_V  T_L  Action name\n",
       "0  750000  -30   990  326  -45  972  181  -38  1000   29       0  S01\n",
       "1  750015  -30  1000  356  -18  981  212  -48  1028   29       0  S01\n",
       "2  750031  -20   990  336   18  981  222  -38  1038    9       0  S01\n",
       "3  750046  -20  1000  316   36  990  222  -19  1038    9       0  S01\n",
       "4  750062    0   990  316   36  990  212  -29  1038   29       0  S01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['S01', 'S02', 'S03', 'S05', 'S06', 'S07', 'S08', 'S09'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(dataset.head())\n",
    "(dataset.name).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SpD-RJzmYfFz"
   },
   "source": [
    "# **To Extract Non-Overlapping windows of length w *f_s  from the continously logged accelerometer data from the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vUKV5l-KT-Bb"
   },
   "outputs": [],
   "source": [
    "def create_window(act,window_length,dataframe):\n",
    "    indices = list(dataframe[dataframe.Action == act].index)\n",
    "    groups = []\n",
    "    temp = []\n",
    "    group_count = 0\n",
    "    for i in range(len(indices)):\n",
    "        if i == len(indices)-1:\n",
    "            temp.append(indices[i])\n",
    "            groups.append(temp)\n",
    "            temp = []\n",
    "            break\n",
    "        temp.append(indices[i])\n",
    "        if indices[i]+1 != indices[i+1]: \n",
    "            group_count+=1\n",
    "            groups.append(temp)\n",
    "            temp = []\n",
    "\n",
    "    fs = 64\n",
    "    window_length = 1\n",
    "    # window_length = window_length*fs\n",
    "\n",
    "    final_dataframe = pd.DataFrame()\n",
    "    for g in groups: \n",
    "        required = math.floor(len(g)/(window_length*fs))\n",
    "        req_index = g[0:(required*fs)]\n",
    "        final_dataframe = pd.concat([final_dataframe,dataframe.iloc[req_index,:]],axis = 0)\n",
    "\n",
    "    return final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1w0cUeeII7AE"
   },
   "outputs": [],
   "source": [
    "for window_length in range(1,5):\n",
    "#     name = \"/Users/vanshika/Downloads/dataset_fog_release/dataset_fog_release/dataset/data.csv\"\n",
    "#     dataframe = pd.read_csv(name)\n",
    "\n",
    "    activities = []\n",
    "    for act in range(3):\n",
    "        activities.append(create_window(act,window_length,dataset))\n",
    "    to_write = pd.concat(activities,axis = 0)\n",
    "    to_write.to_csv(\"/Users/vanshika/Downloads/dataset_fog_release/dataset_fog_release/dataset/window.csv\",index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b_esG9-1hIu-"
   },
   "source": [
    "# Extracting Features\n",
    "\n",
    " The following feature are extracted in the time domain \n",
    " 1. Mean\n",
    " 2. std\n",
    " 3. var\n",
    " 4. Mav\n",
    " 5. rms\n",
    " \n",
    " The following feature are extracted in the frequency  domain \n",
    " 1. Freeze Index\n",
    " 2. Power\n",
    " 3. Energy\n",
    " 4. Entropy\n",
    " 5. Peak Frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L2T7nNv1hIWD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_F\n",
      "A_V\n",
      "A_L\n",
      "L_F\n",
      "L_V\n",
      "L_L\n",
      "T_F\n",
      "T_V\n",
      "T_L\n",
      "(50675, 45)\n"
     ]
    }
   ],
   "source": [
    "#read file \n",
    "window_length = 1\n",
    "fs = 64\n",
    "# for window_length in range(1,5):\n",
    "w = window_length*fs\n",
    "dataframe = pd.read_csv(\"/Users/vanshika/Downloads/dataset_fog_release/dataset_fog_release/dataset/window.csv\")\n",
    "\n",
    "df = dataframe.drop(columns=['time','Action','name'])\n",
    "stat = pd.DataFrame()\n",
    "\n",
    "col= list(df.columns)\n",
    "for s in col:    \n",
    "    print (s)\n",
    "    mn =[] \n",
    "    var = []\n",
    "    std = []\n",
    "    mav = []\n",
    "    rms =[]\n",
    "    for i in range(0,len(df),w):\n",
    "        mn_  = np.mean(df[s].iloc[i:i+w])\n",
    "        var_  = np.var(df[s].iloc[i:i+w])\n",
    "        std_  = np.std(df[s].iloc[i:i+w])\n",
    "        mav_  = np.mean(abs(df[s].iloc[i:i+w]))\n",
    "        rms_  = np.sqrt(np.mean((df[s].iloc[i:i+w])**2))\n",
    "\n",
    "        mn.append(mn_)\n",
    "        var.append(var_)\n",
    "        std.append(std_)\n",
    "        mav.append(mav_)\n",
    "        rms.append(rms_)\n",
    "\n",
    "    stat['mean_'+s] = mn\n",
    "    stat['var_'+s] = var\n",
    "    stat['std_'+s] = std\n",
    "    stat['rms_'+s] = rms\n",
    "    stat['mav_'+s] = mav\n",
    "\n",
    "print(stat.shape)\n",
    "\n",
    "stat1 = copy.copy(stat)\n",
    "stat1['w'] = dataframe['Action'].iloc[[x for x in range(0,len(dataframe),w)]].to_list()\n",
    "order = ['w']\n",
    "order += stat1.columns.to_list()[:-1]\n",
    "stat1 = stat1[order]\n",
    "col = stat1.columns.to_list()\n",
    "col[0] = 0\n",
    "stat1.columns = col\n",
    "feature_name =\"/Users/vanshika/Downloads/dataset_fog_release/dataset_fog_release/dataset/feature.csv\"\n",
    "stat1.to_csv(feature_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 258331,
     "status": "ok",
     "timestamp": 1566927151785,
     "user": {
      "displayName": "Prithvi Suresh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAV2YdYjIVlMX8J9ypPfQapSsaI7t4ILXCOzfTK=s64",
      "userId": "06275891323456735356"
     },
     "user_tz": -330
    },
    "id": "MAdtPuKrJzIJ",
    "outputId": "33493f88-5195-403c-9773-2f9b246f633d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze and power done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vanshika\\AppData\\Local\\Temp/ipykernel_21548/3707615040.py:71: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p_norm = Pxx_den/sum(Pxx_den)\n"
     ]
    }
   ],
   "source": [
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "window_length = 3\n",
    "fs = 64\n",
    "# for window_length in range(1,5):\n",
    "w = window_length*fs\n",
    "dataframe = pd.read_csv(\"/Users/vanshika/Downloads/dataset_fog_release/dataset_fog_release/dataset/window.csv\")\n",
    "\n",
    "df = dataframe.drop(columns=['time','Action','name'])\n",
    "col= list(df.columns)\n",
    "order=5\n",
    "fi=pd.DataFrame()\n",
    "power = pd.DataFrame()\n",
    "\n",
    "bands = {'locomotor' :(0.5,3),'freeze' :(3,8)}\n",
    "\n",
    "for s in col:\n",
    "    xtemp = []\n",
    "    xtemp1 = []\n",
    "    for i in range(0,len(df),w):\n",
    "        nyq=0.5*fs\n",
    "        \n",
    "        #locomotor band 0.5-3hz\n",
    "        loc_low= 0.5/nyq\n",
    "        loc_high=3/nyq\n",
    "        \n",
    "        #clipping off band from the window\n",
    "        b, a = butter(order, [loc_low, loc_high], btype='band')\n",
    "        y=lfilter(b,a,df[s].iloc[i:i+w])\n",
    "        \n",
    "        #total power in locomotor band\n",
    "        e1=sum([x**2 for x in y])\n",
    "\n",
    "        #freeze band 3-8hz\n",
    "        frez_low= 3/nyq\n",
    "        frez_high=8/nyq\n",
    "\n",
    "        #clipping off band from the window\n",
    "        b1, a1 = butter(order, [frez_low, frez_high], btype='band')\n",
    "        y1=lfilter(b1,a1,df[s].iloc[i:i+w])\n",
    "        #total power in locomotor band\n",
    "        e2=sum([x**2 for x in y1])\n",
    "        \n",
    "        FI=e2/e1\n",
    "        POW=e2+e1\n",
    "        xtemp.append(FI)\n",
    "        xtemp1.append(POW)\n",
    "    fi['FI'+s] = xtemp\n",
    "    power['P'+s] = xtemp1\n",
    "print (\"Freeze and power done\")\n",
    "\n",
    "\n",
    "w = window_length*fs\n",
    "E=[]\n",
    "for i in range(0,len(df),w):\n",
    "    energy = np.sum((df.iloc[i:i+w,:])**2)\n",
    "    E.append(energy)\n",
    "E = pd.DataFrame(E)\n",
    "E.columns = [\"EN_\" + x for x in df.columns]\n",
    "\n",
    "#Entropy \n",
    "from scipy.signal import periodogram\n",
    "\n",
    "peak_f = pd.DataFrame()\n",
    "PSE = pd.DataFrame()\n",
    "for s in col:\n",
    "    peakF = []\n",
    "    pse = []\n",
    "    for i in range(0,len(df),w):\n",
    "        f,Pxx_den = periodogram(df[s].iloc[i:i+w],fs)\n",
    "        p_norm = Pxx_den/sum(Pxx_den)\n",
    "        p_norm = list(filter(lambda a: a != 0, p_norm))\n",
    "        pse.append(-(np.sum(p_norm*np.log(p_norm))))\n",
    "        peak = (fs/w)*max(Pxx_den)\n",
    "        peakF.append(peak)\n",
    "    PSE['ENt_'+s] = pse\n",
    "    peak_f['peak_'+s] = peakF\n",
    "PSE.fillna(0,inplace = True)\n",
    "\n",
    "\n",
    "freq = pd.concat([fi,power,E,PSE,peak_f],axis = 1)\n",
    "\n",
    "feature_name = \"/Users/vanshika/Downloads/dataset_fog_release/dataset_fog_release/dataset/frequency.csv\"\n",
    "freq.to_csv(feature_name, index = False)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 604
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 734,
     "status": "ok",
     "timestamp": 1566924398915,
     "user": {
      "displayName": "Prithvi Suresh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAV2YdYjIVlMX8J9ypPfQapSsaI7t4ILXCOzfTK=s64",
      "userId": "06275891323456735356"
     },
     "user_tz": -330
    },
    "id": "Me2x254f7Gs0",
    "outputId": "5e1ec7e4-a2e7-4fcb-ea09-f8b10ac8a015"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 4.54141225e-03, 1.61063550e-03, 9.09157528e-03,\n",
       "       1.60117814e-02, 1.80485091e-02, 7.81984796e-03, 1.95091474e-01,\n",
       "       2.12685394e-02, 3.06752550e-03, 2.68058358e-04, 2.53725397e-02,\n",
       "       2.66419093e-03, 7.96712149e-03, 3.28844817e-02, 7.39991246e-03,\n",
       "       8.79816063e-03, 2.43467941e-02, 1.66178093e-02, 2.33941888e-02,\n",
       "       5.14336962e-03, 7.83044113e-03, 2.56113286e-03, 1.57791311e-02,\n",
       "       1.11091484e-02, 7.42669205e-02, 5.66751455e-02, 3.72423918e-03,\n",
       "       8.99422873e-02, 3.71336491e-03, 8.39543257e-05, 4.24352008e-03,\n",
       "       9.19349017e-03, 4.82559132e-02, 1.01378968e-02, 1.18234610e-02,\n",
       "       1.25245817e-02, 4.24248391e-03, 9.87342825e-04, 7.63097758e-04,\n",
       "       2.14580882e-04, 2.83383722e-03, 3.75990619e-03, 1.02561225e-02,\n",
       "       5.75389723e-03, 1.04469624e-03, 1.15852940e-02, 2.45912174e-03,\n",
       "       4.74789768e-03, 2.00601801e-02, 6.53065357e-03, 1.01566944e-03,\n",
       "       9.66481771e-03, 2.02421954e-03, 2.92205095e-03, 2.17546113e-03,\n",
       "       8.28327912e-04, 1.87475347e-03, 1.02454872e-03, 1.64188485e-03,\n",
       "       7.91341534e-04, 1.21786885e-02, 3.49879556e-03, 7.92321045e-05,\n",
       "       3.05124447e-03, 2.71412991e-03, 5.02696985e-05, 1.93906475e-03,\n",
       "       3.21745742e-03, 1.41748308e-02, 1.84225659e-04, 1.91162481e-03,\n",
       "       1.63954454e-03, 1.54755812e-03, 1.42700128e-03, 2.95573208e-03,\n",
       "       4.66967118e-03, 1.23444035e-03, 1.07664010e-03, 1.60261239e-04,\n",
       "       4.24350395e-05, 9.19513730e-04, 1.84057869e-03, 4.63374283e-04,\n",
       "       5.76915159e-03, 7.10585952e-04, 1.28395613e-03, 5.81862093e-04,\n",
       "       4.10794014e-04, 1.21451810e-04, 1.57797964e-03, 1.28169198e-03,\n",
       "       7.89529197e-04, 8.33023055e-04, 1.35272353e-04, 2.27245384e-03,\n",
       "       3.64219359e-04, 1.14649953e-03, 4.32631652e-04, 3.01568981e-04,\n",
       "       1.12547504e-04, 8.04513033e-05, 1.88063275e-03, 5.00071041e-04,\n",
       "       4.54255920e-03, 2.70797025e-03, 4.26006007e-06, 3.84604682e-04,\n",
       "       2.37421691e-04, 2.99292343e-03, 1.56496576e-03, 3.08421675e-03,\n",
       "       2.24446057e-03, 1.35596870e-03, 8.78045053e-05, 1.49082579e-04,\n",
       "       7.65023590e-04, 9.33891330e-04, 1.23771250e-04, 9.91678403e-05,\n",
       "       2.13064709e-04, 2.71675035e-05, 9.84546613e-04, 1.11098947e-03,\n",
       "       3.25930035e-04, 8.45718319e-05, 1.12666255e-03, 1.60645576e-04,\n",
       "       6.50898671e-04])"
      ]
     },
     "execution_count": 281,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pnfaDYFl-N26"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9k_QktDOd14e",
    "SpD-RJzmYfFz",
    "b_esG9-1hIu-"
   ],
   "name": "DataPreprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
