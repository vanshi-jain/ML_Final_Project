{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZ46dK5XI6_R"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "from pathlib import Path\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/gdrive\", force_remount = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9k_QktDOd14e"
   },
   "source": [
    "# **To Label the Data with 3rd class (Pre-FOG)**\n",
    "Before the occurence of every FOG event, the previous w*f_s timesteps are labelled as a third class 'preFOG' which can be trained in order to predict FOG before it's onset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0yg93EeI6_r"
   },
   "outputs": [],
   "source": [
    "def label_prefog(dataset,window_length = 1):\n",
    "    dataset.drop(index = list(dataset[dataset['Action'] == 0].index),inplace=True) #remove data when person is walking\n",
    "    window_length = 64*window_length #window size for that second(s)\n",
    "    #print(dataset.Action.unique())\n",
    "    fog_index=[]\n",
    "    for i in dataset.index: \n",
    "        if dataset.loc[i,'Action'] == 2:\n",
    "            fog_index.append(i) #add all the instances when fog is occured in that window-size\n",
    "#for indices are the timestamps of the person experiencing fog in that window\n",
    "\n",
    "    start_indices=[]\n",
    "    for i in fog_index:\n",
    "        if (dataset.loc[i-1,'Action']!=dataset.loc[i,'Action']): \n",
    "            # store all the instances of the timestamp before the fog (from label ==1 to label ==2)\n",
    "            start_indices.append(i) #this is the timestamps when the person is starting to face fog (about to)\n",
    "\n",
    "    prefog=[]\n",
    "    for start in start_indices: #for each start instance, store the instances of the previous window\n",
    "        prefog_start = [x for x in range(start-window_length,start)]\n",
    "        prefog.append(prefog_start) \n",
    "        \n",
    "# prefog is for window size=64 * w\n",
    "    prefog = [item for sublist in prefog for item in sublist]\n",
    "# now prefog is for w with each hertz of frequency w/64\n",
    "\n",
    "    for i in prefog:\n",
    "        dataset.loc[i,'Action'] = 3 #mark a new label =3 for each of those instances\n",
    "    #print(dataset.Action.unique())  # 1, 3, 2 where 3 is prefog\n",
    "    dataset['Action'] = dataset['Action'] - 1\n",
    "    #print(dataset.Action.unique())  # 0, 2, 1 where 2 is prefog\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPi05GU1I6_7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R01.txt  is read\tAdding S06R01.txt to dataset\tS06R01.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R02.txt  is read\t\n",
      "S07\n",
      "S07R01.txt  is read\tAdding S07R01.txt to dataset\tS07R01.txt is labelled\n",
      "\n",
      "S07\n",
      "S07R02.txt  is read\tAdding S07R02.txt to dataset\tS07R02.txt is labelled\n",
      "\n",
      "S08\n",
      "S08R01.txt  is read\tAdding S08R01.txt to dataset\tS08R01.txt is labelled\n",
      "\n",
      "S09\n",
      "S09R01.txt  is read\tAdding S09R01.txt to dataset\tS09R01.txt is labelled\n",
      "\n",
      "S10\n",
      "S10R01.txt  is read\t\n",
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R01.txt  is read\tAdding S06R01.txt to dataset\tS06R01.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R02.txt  is read\t\n",
      "S07\n",
      "S07R01.txt  is read\tAdding S07R01.txt to dataset\tS07R01.txt is labelled\n",
      "\n",
      "S07\n",
      "S07R02.txt  is read\tAdding S07R02.txt to dataset\tS07R02.txt is labelled\n",
      "\n",
      "S08\n",
      "S08R01.txt  is read\tAdding S08R01.txt to dataset\tS08R01.txt is labelled\n",
      "\n",
      "S09\n",
      "S09R01.txt  is read\tAdding S09R01.txt to dataset\tS09R01.txt is labelled\n",
      "\n",
      "S10\n",
      "S10R01.txt  is read\t\n",
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R01.txt  is read\tAdding S06R01.txt to dataset\tS06R01.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R02.txt  is read\t\n",
      "S07\n",
      "S07R01.txt  is read\tAdding S07R01.txt to dataset\tS07R01.txt is labelled\n",
      "\n",
      "S07\n",
      "S07R02.txt  is read\tAdding S07R02.txt to dataset\tS07R02.txt is labelled\n",
      "\n",
      "S08\n",
      "S08R01.txt  is read\tAdding S08R01.txt to dataset\tS08R01.txt is labelled\n",
      "\n",
      "S09\n",
      "S09R01.txt  is read\tAdding S09R01.txt to dataset\tS09R01.txt is labelled\n",
      "\n",
      "S10\n",
      "S10R01.txt  is read\t\n",
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R01.txt  is read\tAdding S06R01.txt to dataset\tS06R01.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R02.txt  is read\t\n",
      "S07\n",
      "S07R01.txt  is read\tAdding S07R01.txt to dataset\tS07R01.txt is labelled\n",
      "\n",
      "S07\n",
      "S07R02.txt  is read\tAdding S07R02.txt to dataset\tS07R02.txt is labelled\n",
      "\n",
      "S08\n",
      "S08R01.txt  is read\tAdding S08R01.txt to dataset\tS08R01.txt is labelled\n",
      "\n",
      "S09\n",
      "S09R01.txt  is read\tAdding S09R01.txt to dataset\tS09R01.txt is labelled\n",
      "\n",
      "S10\n",
      "S10R01.txt  is read\t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3298672, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"/Users/vanshika/Downloads/dataset_fog_release/dataset_fog_release/dataset\"\n",
    "#\" Enter the folder path of Unzipped UCI Daphnet Dataset \"\n",
    "\n",
    "people = []\n",
    "dataset=pd.DataFrame()\n",
    "for person in os.listdir(data_path):\n",
    "    if '.txt' in person: \n",
    "        people.append(person)\n",
    "        \n",
    "for window_length in range(1,5):\n",
    "    for person in people: \n",
    "        name = person.split('R')[0]\n",
    "        print (name)\n",
    "        file = data_path+\"/\"+person\n",
    "        temp = pd.read_csv(file,delimiter= \" \", header = None)\n",
    "        print (person,' is read',end = '\\t')\n",
    "\n",
    "        if 2 in temp[max(temp.columns)].unique(): #that patient/ person has undergone FoG == 2\n",
    "            print ('Adding {} to dataset'.format(person),end = '\\t')            \n",
    "            temp.columns = ['time','A_F','A_V','A_L','L_F','L_V','L_L','T_F','T_V','T_L','Action']   \n",
    "        #    print(temp.Action.unique())\n",
    "            temp = label_prefog(temp,window_length).reset_index(drop=True)\n",
    "\n",
    "            temp['name'] = name\n",
    "            \n",
    "            print ('{} is labelled'.format(person))\n",
    "            dataset = pd.concat([dataset,temp],axis = 0)\n",
    "\n",
    "        print ('')\n",
    "    dataset.reset_index(drop =True,inplace=True) \n",
    "    dataset.to_csv(\"/Users/vanshika/Downloads/dataset_fog_release/dataset_fog_release/dataset/data.csv\",index = None)\n",
    "\n",
    "\n",
    "display(dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>A_F</th>\n",
       "      <th>A_V</th>\n",
       "      <th>A_L</th>\n",
       "      <th>L_F</th>\n",
       "      <th>L_V</th>\n",
       "      <th>L_L</th>\n",
       "      <th>T_F</th>\n",
       "      <th>T_V</th>\n",
       "      <th>T_L</th>\n",
       "      <th>Action</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>750000</td>\n",
       "      <td>-30</td>\n",
       "      <td>990</td>\n",
       "      <td>326</td>\n",
       "      <td>-45</td>\n",
       "      <td>972</td>\n",
       "      <td>181</td>\n",
       "      <td>-38</td>\n",
       "      <td>1000</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>750015</td>\n",
       "      <td>-30</td>\n",
       "      <td>1000</td>\n",
       "      <td>356</td>\n",
       "      <td>-18</td>\n",
       "      <td>981</td>\n",
       "      <td>212</td>\n",
       "      <td>-48</td>\n",
       "      <td>1028</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>750031</td>\n",
       "      <td>-20</td>\n",
       "      <td>990</td>\n",
       "      <td>336</td>\n",
       "      <td>18</td>\n",
       "      <td>981</td>\n",
       "      <td>222</td>\n",
       "      <td>-38</td>\n",
       "      <td>1038</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>750046</td>\n",
       "      <td>-20</td>\n",
       "      <td>1000</td>\n",
       "      <td>316</td>\n",
       "      <td>36</td>\n",
       "      <td>990</td>\n",
       "      <td>222</td>\n",
       "      <td>-19</td>\n",
       "      <td>1038</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>750062</td>\n",
       "      <td>0</td>\n",
       "      <td>990</td>\n",
       "      <td>316</td>\n",
       "      <td>36</td>\n",
       "      <td>990</td>\n",
       "      <td>212</td>\n",
       "      <td>-29</td>\n",
       "      <td>1038</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>S01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     time  A_F   A_V  A_L  L_F  L_V  L_L  T_F   T_V  T_L  Action name\n",
       "0  750000  -30   990  326  -45  972  181  -38  1000   29       0  S01\n",
       "1  750015  -30  1000  356  -18  981  212  -48  1028   29       0  S01\n",
       "2  750031  -20   990  336   18  981  222  -38  1038    9       0  S01\n",
       "3  750046  -20  1000  316   36  990  222  -19  1038    9       0  S01\n",
       "4  750062    0   990  316   36  990  212  -29  1038   29       0  S01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(array(['S01', 'S02', 'S03', 'S05', 'S06', 'S07', 'S08', 'S09'],\n",
       "       dtype=object),\n",
       " array([0, 2, 1], dtype=int64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(dataset.head())\n",
    "(dataset.name).unique(), dataset.Action.unique() #person S04 and S10 do not undergo FoG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SpD-RJzmYfFz"
   },
   "source": [
    "# **To Extract Non-Overlapping windows of length w *f_s  from the continously logged accelerometer data from the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vUKV5l-KT-Bb"
   },
   "outputs": [],
   "source": [
    "def create_window(act,window_length,dataframe):\n",
    "    indices = list(dataframe[dataframe.Action == act].index)\n",
    "    groups = []\n",
    "    temp = []\n",
    "    group_count = 0\n",
    "    for i in range(len(indices)):\n",
    "        if i == len(indices)-1:\n",
    "            temp.append(indices[i])\n",
    "            groups.append(temp)\n",
    "            temp = []\n",
    "            break\n",
    "        temp.append(indices[i])\n",
    "        if indices[i]+1 != indices[i+1]: \n",
    "            group_count+=1\n",
    "            groups.append(temp)\n",
    "            temp = []\n",
    "\n",
    "    fs = 64\n",
    "    window_length = 1\n",
    "    # window_length = window_length*fs\n",
    "\n",
    "    final_dataframe = pd.DataFrame()\n",
    "    for g in groups: \n",
    "        required = math.floor(len(g)/(window_length*fs))\n",
    "        req_index = g[0:(required*fs)]\n",
    "        final_dataframe = pd.concat([final_dataframe,dataframe.iloc[req_index,:]],axis = 0)\n",
    "\n",
    "    return final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1w0cUeeII7AE"
   },
   "outputs": [],
   "source": [
    "for window_length in range(1,5):\n",
    "    activities = []\n",
    "    for act in range(3):\n",
    "        activities.append(create_window(act,window_length,dataset))\n",
    "    to_write = pd.concat(activities,axis = 0)\n",
    "    to_write.to_csv(\"/Users/vanshika/Downloads/dataset_fog_release/dataset_fog_release/dataset/window.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b_esG9-1hIu-"
   },
   "source": [
    "# Extracting Features\n",
    "\n",
    " The following feature are extracted in the time domain \n",
    " 1. Mean\n",
    " 2. std\n",
    " 3. var\n",
    " 4. Mav\n",
    " 5. rms\n",
    " \n",
    " The following feature are extracted in the frequency  domain \n",
    " 1. Freeze Index\n",
    " 2. Power\n",
    " 3. Energy\n",
    " 4. Entropy\n",
    " 5. Peak Frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L2T7nNv1hIWD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_F\n",
      "A_V\n",
      "A_L\n",
      "L_F\n",
      "L_V\n",
      "L_L\n",
      "T_F\n",
      "T_V\n",
      "T_L\n",
      "(50675, 45)\n"
     ]
    }
   ],
   "source": [
    "#compute time-domain features for each window\n",
    "window_length = 1\n",
    "fs = 64\n",
    "w = window_length*fs\n",
    "dataframe = pd.read_csv(\"/Users/vanshika/Downloads/dataset_fog_release/dataset_fog_release/dataset/window.csv\")\n",
    "\n",
    "df = dataframe.drop(columns=['time','Action','name'])\n",
    "stat = pd.DataFrame()\n",
    "col= list(df.columns)\n",
    "#for each sensor column, compute the features\n",
    "for s in col:    \n",
    "    print (s)\n",
    "    mn =[] \n",
    "    var = []\n",
    "    std = []\n",
    "    mav = []\n",
    "    rms =[]\n",
    "    for i in range(0,len(df),w):\n",
    "        mn_  = np.mean(df[s].iloc[i:i+w])\n",
    "        var_  = np.var(df[s].iloc[i:i+w])\n",
    "        std_  = np.std(df[s].iloc[i:i+w])\n",
    "        mav_  = np.mean(abs(df[s].iloc[i:i+w]))\n",
    "        rms_  = np.sqrt(np.mean((df[s].iloc[i:i+w])**2))\n",
    "\n",
    "        mn.append(mn_)\n",
    "        var.append(var_)\n",
    "        std.append(std_)\n",
    "        mav.append(mav_)\n",
    "        rms.append(rms_)\n",
    "# make a new column for each feature for each sensor- 9 * 5 = 45 features\n",
    "    stat['mean_'+s] = mn\n",
    "    stat['var_'+s] = var\n",
    "    stat['std_'+s] = std\n",
    "    stat['rms_'+s] = rms\n",
    "    stat['mav_'+s] = mav\n",
    "\n",
    "print(stat.shape)\n",
    "\n",
    "stat1 = copy.copy(stat)\n",
    "stat1['w'] = dataframe['Action'].iloc[[x for x in range(0,len(dataframe),w)]].to_list()\n",
    "order = ['w']\n",
    "order += stat1.columns.to_list()[:-1]\n",
    "stat1 = stat1[order]\n",
    "col = stat1.columns.to_list()\n",
    "col[0] = 0\n",
    "stat1.columns = col\n",
    "feature_name =\"/Users/vanshika/Downloads/dataset_fog_release/dataset_fog_release/dataset/time.csv\"\n",
    "stat1.to_csv(feature_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50675, 46)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat1.shape # adding labels inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 258331,
     "status": "ok",
     "timestamp": 1566927151785,
     "user": {
      "displayName": "Prithvi Suresh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAV2YdYjIVlMX8J9ypPfQapSsaI7t4ILXCOzfTK=s64",
      "userId": "06275891323456735356"
     },
     "user_tz": -330
    },
    "id": "MAdtPuKrJzIJ",
    "outputId": "33493f88-5195-403c-9773-2f9b246f633d"
   },
   "outputs": [],
   "source": [
    "# While including frequency domain features in our model\n",
    "# yields better recall score, it is observed in some research papers\n",
    "# that the latency due to these features compared to itâ€™s\n",
    "# time domain counterpart is very high. This is attributed to the\n",
    "# simplicity of time domain features which are based on simple\n",
    "# mathematical and statistical operations whereas the frequency\n",
    "# domain needs performance of Fast Fourier Transform (FFT)\n",
    "\n",
    "# #frquency domain features\n",
    "# from scipy.signal import butter, lfilter\n",
    "\n",
    "# window_length = 1\n",
    "# fs = 64\n",
    "# w = window_length*fs\n",
    "# dataframe = pd.read_csv(\"/Users/vanshika/Downloads/dataset_fog_release/dataset_fog_release/dataset/window.csv\")\n",
    "\n",
    "# df = dataframe.drop(columns=['time','Action','name'])\n",
    "# col= list(df.columns)\n",
    "# order=5\n",
    "# fi=pd.DataFrame()\n",
    "# power = pd.DataFrame()\n",
    "\n",
    "# bands = {'locomotor' :(0.5,3),'freeze' :(3,8)} #data from parkinson-gait paper\n",
    "\n",
    "# for s in col:\n",
    "#     xtemp = []\n",
    "#     xtemp1 = []\n",
    "#     for i in range(0,len(df),w):\n",
    "#         nyq=0.5*fs\n",
    "        \n",
    "#         #locomotor band 0.5-3hz\n",
    "#         loc_low= 0.5/nyq\n",
    "#         loc_high=3/nyq\n",
    "        \n",
    "#         #clipping off band from the window\n",
    "#         b, a = butter(order, [loc_low, loc_high], btype='band')\n",
    "#         y=lfilter(b,a,df[s].iloc[i:i+w])\n",
    "        \n",
    "#         #total power in locomotor band\n",
    "#         e1=sum([x**2 for x in y])\n",
    "\n",
    "#         #freeze band 3-8hz\n",
    "#         frez_low= 3/nyq\n",
    "#         frez_high=8/nyq\n",
    "\n",
    "#         #clipping off band from the window\n",
    "#         b1, a1 = butter(order, [frez_low, frez_high], btype='band')\n",
    "#         y1=lfilter(b1,a1,df[s].iloc[i:i+w])\n",
    "#         #total power in locomotor band\n",
    "#         e2=sum([x**2 for x in y1])\n",
    "        \n",
    "#         FI=e2/e1\n",
    "#         POW=e2+e1\n",
    "#         xtemp.append(FI)\n",
    "#         xtemp1.append(POW)\n",
    "#     fi['FI'+s] = xtemp\n",
    "#     power['P'+s] = xtemp1\n",
    "# print (\"Freeze and power done\")\n",
    "\n",
    "\n",
    "# w = window_length*fs\n",
    "# E=[]\n",
    "# for i in range(0,len(df),w):\n",
    "#     energy = np.sum((df.iloc[i:i+w,:])**2)\n",
    "#     E.append(energy)\n",
    "# E = pd.DataFrame(E)\n",
    "# E.columns = [\"EN_\" + x for x in df.columns]\n",
    "\n",
    "# #Entropy \n",
    "# from scipy.signal import periodogram\n",
    "\n",
    "# peak_f = pd.DataFrame()\n",
    "# PSE = pd.DataFrame()\n",
    "# for s in col:\n",
    "#     peakF = []\n",
    "#     pse = []\n",
    "#     for i in range(0,len(df),w):\n",
    "#         f,Pxx_den = periodogram(df[s].iloc[i:i+w],fs)\n",
    "#         p_norm = Pxx_den/sum(Pxx_den)\n",
    "#         p_norm = list(filter(lambda a: a != 0, p_norm))\n",
    "#         pse.append(-(np.sum(p_norm*np.log(p_norm))))\n",
    "#         peak = (fs/w)*max(Pxx_den)\n",
    "#         peakF.append(peak)\n",
    "#     PSE['ENt_'+s] = pse\n",
    "#     peak_f['peak_'+s] = peakF\n",
    "# PSE.fillna(0,inplace = True)\n",
    "\n",
    "\n",
    "# freq = pd.concat([fi,power,E,PSE,peak_f],axis = 1)\n",
    "\n",
    "# feature_name = \"/Users/vanshika/Downloads/dataset_fog_release/dataset_fog_release/dataset/frequency.csv\"\n",
    "# freq.to_csv(feature_name, index = False)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9k_QktDOd14e",
    "SpD-RJzmYfFz",
    "b_esG9-1hIu-"
   ],
   "name": "DataPreprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
