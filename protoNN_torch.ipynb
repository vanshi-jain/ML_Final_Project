{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a52bf0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import subprocess\n",
    "import glob\n",
    "import scipy.cluster\n",
    "import scipy.spatial\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6c1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# antlr4-python3-runtime==4.7\n",
    "# jupyter==1.0.0\n",
    "# numpy==1.16.4\n",
    "# pandas==0.23.4\n",
    "# scikit-learn==0.21.2\n",
    "# scipy==1.3.0\n",
    "# torch==1.4.0\n",
    "# torchvision==0.5.0\n",
    "# requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df2dafad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def findCUDA():\n",
    "#     '''Finds the CUDA install path.'''\n",
    "#     # Guess #1\n",
    "#     IS_WINDOWS = sys.platform == 'win32'\n",
    "#     cuda_home =None# os.environ.get('CUDA_HOME') or os.environ.get('CUDA_PATH')\n",
    "#     if cuda_home is None:\n",
    "#         # Guess #2\n",
    "#         try:\n",
    "#             which = 'where' if IS_WINDOWS else 'which'\n",
    "#             nvcc = subprocess.check_output(\n",
    "#                 [which, 'nvcc']).decode().rstrip('\\r\\n')\n",
    "#             cuda_home = os.path.dirname(os.path.dirname(nvcc))\n",
    "#         except Exception:\n",
    "#             # Guess #3\n",
    "#             if IS_WINDOWS:\n",
    "#                 cuda_homes = glob.glob(\n",
    "#                     'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v*.*')\n",
    "#                 if len(cuda_homes) == 0:\n",
    "#                     cuda_home = ''\n",
    "#                 else:\n",
    "#                     cuda_home = cuda_homes[0]\n",
    "#             else:\n",
    "#                 cuda_home = '/usr/local/cuda'\n",
    "#             if not os.path.exists(cuda_home):\n",
    "#                 cuda_home = None\n",
    "#     return cuda_home\n",
    "\n",
    "def multiClassHingeLoss(logits, labels):\n",
    "    '''\n",
    "    MultiClassHingeLoss to match C++ Version - No pytorch internal version\n",
    "    '''\n",
    "    flatLogits = torch.reshape(logits, [-1, ])\n",
    "    labels_ = labels.argmax(dim=1)\n",
    "\n",
    "    correctId = torch.arange(labels.shape[0]).to(\n",
    "        logits.device) * labels.shape[1] + labels_\n",
    "    correctLogit = torch.gather(flatLogits, 0, correctId)\n",
    "\n",
    "    maxLabel = logits.argmax(dim=1)\n",
    "    top2, _ = torch.topk(logits, k=2, sorted=True)\n",
    "\n",
    "    wrongMaxLogit = torch.where((maxLabel == labels_), top2[:, 1], top2[:, 0])\n",
    "\n",
    "    return torch.mean(F.relu(1. + wrongMaxLogit - correctLogit))\n",
    "\n",
    "\n",
    "def crossEntropyLoss(logits, labels):\n",
    "    '''\n",
    "    Cross Entropy loss for MultiClass case in joint training for\n",
    "    faster convergence\n",
    "    '''\n",
    "    return F.cross_entropy(logits, labels.argmax(dim=1))\n",
    "\n",
    "\n",
    "def binaryHingeLoss(logits, labels):\n",
    "    '''\n",
    "    BinaryHingeLoss to match C++ Version - No pytorch internal version\n",
    "    '''\n",
    "    return torch.mean(F.relu(1.0 - (2 * labels - 1) * logits))\n",
    "\n",
    "\n",
    "def hardThreshold(A: torch.Tensor, s):\n",
    "    '''\n",
    "    Hard thresholds and modifies in-palce nn.Parameter A with sparsity s \n",
    "    '''\n",
    "    #PyTorch disallows numpy access/copy to tensors in graph.\n",
    "    #.detach() creates a new tensor not attached to the graph.\n",
    "    A_ = A.data.cpu().detach().numpy().ravel()    \n",
    "    if len(A_) > 0:\n",
    "        th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n",
    "        A_[np.abs(A_) < th] = 0.0\n",
    "    A_ = A_.reshape(A.shape)\n",
    "    return torch.tensor(A_, requires_grad=True)\n",
    "\n",
    "def supportBasedThreshold(dst: torch.Tensor, src: torch.Tensor):\n",
    "    '''\n",
    "    zero out entries in dst.data that are zeros in src tensor\n",
    "    '''\n",
    "    return copySupport(src, dst.data)\n",
    "\n",
    "def copySupport(src, dst):\n",
    "    '''\n",
    "    zero out entries in dst.data that are zeros in src tensor\n",
    "    '''\n",
    "    zeroSupport = (src.view(-1) == 0.0).nonzero()\n",
    "    dst = dst.reshape(-1)\n",
    "    dst[zeroSupport] = 0\n",
    "    dst = dst.reshape(src.shape)\n",
    "    del zeroSupport\n",
    "    return dst\n",
    "\n",
    "\n",
    "def estimateNNZ(A, s, bytesPerVar=4):\n",
    "    '''\n",
    "    Returns # of non-zeros and representative size of the tensor\n",
    "    Uses dense for s >= 0.5 - 4 byte\n",
    "    Else uses sparse - 8 byte\n",
    "    '''\n",
    "    params = 1\n",
    "    hasSparse = False\n",
    "    for i in range(0, len(A.shape)):\n",
    "        params *= int(A.shape[i])\n",
    "    if s < 0.5:\n",
    "        nnZ = np.ceil(params * s)\n",
    "        hasSparse = True\n",
    "        return nnZ, nnZ * 2 * bytesPerVar, hasSparse\n",
    "    else:\n",
    "        nnZ = params\n",
    "        return nnZ, nnZ * bytesPerVar, hasSparse\n",
    "\n",
    "\n",
    "def countNNZ(A: torch.nn.Parameter, isSparse):\n",
    "    '''\n",
    "    Returns # of non-zeros \n",
    "    '''\n",
    "    A_ = A.detach().numpy()\n",
    "    if isSparse:\n",
    "        return np.count_nonzero(A_)\n",
    "    else:\n",
    "        nnzs = 1\n",
    "        for i in range(0, len(A.shape)):\n",
    "            nnzs *= int(A.shape[i])\n",
    "        return nnzs\n",
    "\n",
    "def restructreMatrixBonsaiSeeDot(A, nClasses, nNodes):\n",
    "    '''\n",
    "    Restructures a matrix from [nNodes*nClasses, Proj] to\n",
    "    [nClasses*nNodes, Proj] for SeeDot\n",
    "    '''\n",
    "    tempMatrix = np.zeros(A.shape)\n",
    "    rowIndex = 0\n",
    "\n",
    "    for i in range(0, nClasses):\n",
    "        for j in range(0, nNodes):\n",
    "            tempMatrix[rowIndex] = A[j * nClasses + i]\n",
    "            rowIndex += 1\n",
    "\n",
    "    return tempMatrix\n",
    "\n",
    "class TriangularLR(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, stepsize, lr_min, lr_max, gamma):\n",
    "        self.stepsize = stepsize\n",
    "        self.lr_min = lr_min\n",
    "        self.lr_max = lr_max\n",
    "        self.gamma = gamma\n",
    "        super(TriangularLR, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        it = self.last_epoch\n",
    "        cycle = math.floor(1 + it / (2 * self.stepsize))\n",
    "        x = abs(it / self.stepsize - 2 * cycle + 1)\n",
    "        decayed_range = (self.lr_max - self.lr_min) * self.gamma ** (it / 3)\n",
    "        lr = self.lr_min + decayed_range * x\n",
    "        return [lr]\n",
    "\n",
    "class ExponentialResettingLR(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, gamma, reset_epoch):\n",
    "        self.gamma = gamma\n",
    "        self.reset_epoch = int(reset_epoch)\n",
    "        super(ExponentialResettingLR, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        epoch = self.last_epoch\n",
    "        if epoch > self.reset_epoch:\n",
    "            epoch -= self.reset_epoch\n",
    "        return [base_lr * self.gamma ** epoch\n",
    "                for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f94d4cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoNN(nn.Module):\n",
    "    def __init__(self, inputDimension, projectionDimension, numPrototypes,\n",
    "                 numOutputLabels, gamma, W=None, B=None, Z=None):\n",
    "        '''\n",
    "        Forward computation graph for ProtoNN.\n",
    "        inputDimension: Input data dimension or feature dimension.\n",
    "        projectionDimension: hyperparameter\n",
    "        numPrototypes: hyperparameter\n",
    "        numOutputLabels: The number of output labels or classes\n",
    "        W, B, Z: Numpy matrices that can be used to initialize\n",
    "            projection matrix(W), prototype matrix (B) and prototype labels\n",
    "            matrix (B).\n",
    "            Expected Dimensions:\n",
    "                W   inputDimension (d) x projectionDimension (d_cap)\n",
    "                B   projectionDimension (d_cap) x numPrototypes (m)\n",
    "                Z   numOutputLabels (L) x numPrototypes (m)\n",
    "        '''\n",
    "        super(ProtoNN, self).__init__()\n",
    "        self.__d = inputDimension\n",
    "        self.__d_cap = projectionDimension\n",
    "        self.__m = numPrototypes\n",
    "        self.__L = numOutputLabels\n",
    "\n",
    "        self.W, self.B, self.Z = None, None, None\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.__validInit = False\n",
    "        self.__initWBZ(W, B, Z)\n",
    "        self.__validateInit()\n",
    "\n",
    "    def __validateInit(self):\n",
    "        self.__validinit = False\n",
    "        errmsg = \"Dimensions mismatch! Should be W[d, d_cap]\"\n",
    "        errmsg+= \", B[d_cap, m] and Z[L, m]\"\n",
    "        d, d_cap, m, L, _ = self.getHyperParams()\n",
    "        assert self.W.shape[0] == d, errmsg\n",
    "        assert self.W.shape[1] == d_cap, errmsg\n",
    "        assert self.B.shape[0] == d_cap, errmsg\n",
    "        assert self.B.shape[1] == m, errmsg\n",
    "        assert self.Z.shape[0] == L, errmsg\n",
    "        assert self.Z.shape[1] == m, errmsg\n",
    "        self.__validInit = True\n",
    "\n",
    "    def __initWBZ(self, inW, inB, inZ):\n",
    "        if inW is None:\n",
    "            self.W = torch.randn([self.__d, self.__d_cap])\n",
    "            self.W = nn.Parameter(self.W)\n",
    "        else:\n",
    "            self.W = nn.Parameter(torch.from_numpy(inW.astype(np.float32)))\n",
    "\n",
    "        if inB is None:\n",
    "            self.B = torch.randn([self.__d_cap, self.__m])\n",
    "            self.B = nn.Parameter(self.B)\n",
    "        else:\n",
    "            self.B = nn.Parameter(torch.from_numpy(inB.astype(np.float32)))\n",
    "\n",
    "        if inZ is None:\n",
    "            self.Z = torch.randn([self.__L, self.__m])\n",
    "            self.Z = nn.Parameter(self.Z)\n",
    "        else:\n",
    "            self.Z = nn.Parameter(torch.from_numpy(inZ.astype(np.float32)))\n",
    "\n",
    "    def getHyperParams(self):\n",
    "        '''\n",
    "        Returns the model hyperparameters:\n",
    "            [inputDimension, projectionDimension, numPrototypes,\n",
    "            numOutputLabels, gamma]\n",
    "        '''\n",
    "        d =  self.__d\n",
    "        dcap = self.__d_cap\n",
    "        m = self.__m\n",
    "        L = self.__L\n",
    "        return d, dcap, m, L, self.gamma\n",
    "\n",
    "    def getModelMatrices(self):\n",
    "        '''\n",
    "        Returns model matrices, which can then be evaluated to obtain\n",
    "        corresponding numpy arrays.  These can then be exported as part of\n",
    "        other implementations of ProtonNN, for instance a C++ implementation or\n",
    "        pure python implementation.\n",
    "        Returns\n",
    "            [ProjectionMatrix (W), prototypeMatrix (B),\n",
    "             prototypeLabelsMatrix (Z), gamma]\n",
    "        '''\n",
    "        return self.W, self.B, self.Z, self.gamma\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        This method is responsible for construction of the forward computation\n",
    "        graph. The end point of the computation graph, or in other words the\n",
    "        output operator for the forward computation is returned.\n",
    "        X: Input of shape [-1, inputDimension]\n",
    "        returns: The forward computation outputs, self.protoNNOut\n",
    "        '''\n",
    "        assert self.__validInit is True, \"Initialization failed!\"\n",
    "\n",
    "        W, B, Z, gamma = self.W, self.B, self.Z, self.gamma\n",
    "        WX = torch.matmul(X, W)\n",
    "        dim = [-1, WX.shape[1], 1]\n",
    "        WX = torch.reshape(WX, dim)\n",
    "        dim = [1, B.shape[0], -1]\n",
    "        B_ = torch.reshape(B, dim)\n",
    "        l2sim = B_ - WX\n",
    "        l2sim = torch.pow(l2sim, 2)\n",
    "        l2sim = torch.sum(l2sim, dim=1, keepdim=True)\n",
    "        self.l2sim = l2sim\n",
    "        gammal2sim = (-1 * gamma * gamma) * l2sim\n",
    "        M = torch.exp(gammal2sim)\n",
    "        dim = [1] + list(Z.shape)\n",
    "        Z_ = torch.reshape(Z, dim)\n",
    "        y = Z_ * M\n",
    "        y = torch.sum(y, dim=2)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8411cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoNNTrainer:\n",
    "\n",
    "    def __init__(self, protoNNObj, regW, regB, regZ, sparcityW, sparcityB,\n",
    "                 sparcityZ, learningRate, lossType='l2', device=None):\n",
    "        '''\n",
    "        A wrapper for the various techniques used for training ProtoNN. This\n",
    "        subsumes both the responsibility of loss graph construction and\n",
    "        performing training. The original training routine that is part of the\n",
    "        C++ implementation of EdgeML used iterative hard thresholding (IHT),\n",
    "        gamma estimation through median heuristic and other tricks for\n",
    "        training ProtoNN. This module implements the same in pytorch\n",
    "        and python.\n",
    "        protoNNObj: An instance of ProtoNN class defining the forward\n",
    "            computation graph. The loss functions and training routines will be\n",
    "            attached to this instance.\n",
    "        regW, regB, regZ: Regularization constants for W, B, and\n",
    "            Z matrices of protoNN.\n",
    "        sparcityW, sparcityB, sparcityZ: Sparsity constraints\n",
    "            for W, B and Z matrices. A value between 0 (exclusive) and 1\n",
    "            (inclusive) is expected. A value of 1 indicates dense training.\n",
    "        learningRate: Initial learning rate for ADAM optimizer.\n",
    "        X, Y : Placeholders for data and labels.\n",
    "            X [-1, featureDimension]\n",
    "            Y [-1, num Labels]\n",
    "        lossType: ['l2', 'xentropy']\n",
    "        '''\n",
    "        self.protoNNObj = protoNNObj\n",
    "        self.__regW = regW\n",
    "        self.__regB = regB\n",
    "        self.__regZ = regZ\n",
    "        self.__sW = sparcityW\n",
    "        self.__sB = sparcityB\n",
    "        self.__sZ = sparcityZ\n",
    "        self.__lR = learningRate\n",
    "        self.sparseTraining = True\n",
    "        if (sparcityW == 1.0) and (sparcityB == 1.0) and (sparcityZ == 1.0):\n",
    "            self.sparseTraining = False\n",
    "            print(\"Sparse training disabled.\", file=sys.stderr)\n",
    "        self.W_th = None\n",
    "        self.B_th = None\n",
    "        self.Z_th = None\n",
    "        self.__lossType = lossType\n",
    "        self.optimizer = self.__optimizer()\n",
    "        self.lossCriterion = None\n",
    "        #assert lossType in ['l2', 'xentropy']\n",
    "        if lossType == 'l2':\n",
    "            self.lossCriterion = torch.nn.MSELoss()\n",
    "            print(\"Using L2 (MSE) loss\")\n",
    "        else :\n",
    "            self.lossCriterion = torch.nn.CrossEntropyLoss()\n",
    "            print(\"Using x-entropy loss\")\n",
    "        self.__validInit = False\n",
    "        self.__validInit = self.__validateInit()\n",
    "        if device is None:\n",
    "            self.device = \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "    def __validateInit(self):\n",
    "        assert self.__validInit == False\n",
    "        msg = \"Sparsity values should be between 0 and 1 (both inclusive)\"\n",
    "        assert 0 <= self.__sW <= 1, msg\n",
    "        assert 0 <= self.__sB <= 1, msg\n",
    "        assert 0 <= self.__sZ <= 1, msg\n",
    "        return True\n",
    "\n",
    "    def __optimizer(self):\n",
    "        optimizer = torch.optim.Adam(self.protoNNObj.parameters(),\n",
    "                                     lr=self.__lR)\n",
    "        return optimizer\n",
    "\n",
    "    def loss(self, logits, labels_or_target):\n",
    "        labels = labels_or_target\n",
    "        assert len(logits) == len(labels)\n",
    "        assert len(labels.shape) == 2\n",
    "        assert len(logits.shape) == 2\n",
    "        regLoss = (self.__regW * (torch.norm(self.protoNNObj.W)**2) +\n",
    "                   self.__regB * (torch.norm(self.protoNNObj.B)**2) +\n",
    "                   self.__regZ * (torch.norm(self.protoNNObj.Z)**2))\n",
    "        if self.__lossType == 'xentropy':\n",
    "            _, labels = torch.max(labels, dim=1)\n",
    "            assert len(labels.shape)== 1\n",
    "        loss = self.lossCriterion(logits, labels) + regLoss\n",
    "        return loss\n",
    "\n",
    "    def accuracy(self, predictions, labels):\n",
    "        '''\n",
    "        Returns accuracy and number of correct predictions.\n",
    "        '''\n",
    "        assert len(predictions.shape) == 1\n",
    "        assert len(labels.shape) == 1\n",
    "        assert len(predictions) == len(labels)\n",
    "        correct = (predictions == labels).double()\n",
    "        numCorrect = torch.sum(correct)\n",
    "        acc = torch.mean(correct)\n",
    "        return acc, numCorrect\n",
    "\n",
    "    def hardThreshold(self):\n",
    "        prtn = self.protoNNObj\n",
    "        W, B, Z = prtn.W.data, prtn.B.data, prtn.Z.data\n",
    "        newW = hardThreshold(W, self.__sW)\n",
    "        newB = hardThreshold(B, self.__sB)\n",
    "        newZ = hardThreshold(Z, self.__sZ)\n",
    "        prtn.W.data = torch.FloatTensor(newW).to(self.device)\n",
    "        prtn.B.data = torch.FloatTensor(newB).to(self.device)\n",
    "        prtn.Z.data = torch.FloatTensor(newZ).to(self.device)\n",
    "\n",
    "    def train(self, batchSize, epochs, x_train, x_val, y_train, y_val,\n",
    "              printStep=10, valStep=1):\n",
    "        '''\n",
    "        Performs dense training of ProtoNN followed by iterative hard\n",
    "        thresholding to enforce sparsity constraints.\n",
    "        batchSize: Batch size per update\n",
    "        epochs : The number of epochs to run training for. One epoch is\n",
    "            defined as one pass over the entire training data.\n",
    "        x_train, x_val, y_train, y_val: The numpy array containing train and\n",
    "            validation data. x data is assumed to in of shape [-1,\n",
    "            featureDimension] while y should have shape [-1, numberLabels].\n",
    "        printStep: Number of batches between echoing of loss and train accuracy.\n",
    "        valStep: Number of epochs between evaluations on validation set.\n",
    "        '''\n",
    "        d, dcap, m, L, _ = self.protoNNObj.getHyperParams()\n",
    "        assert batchSize >= 1, 'Batch size should be positive integer'\n",
    "        assert epochs >= 1, 'Total epochs should be positive integer'\n",
    "        assert x_train.ndim == 2, 'Expected training data to be of rank 2'\n",
    "        assert x_train.shape[1] == d, 'Expected x_train to be [-1, %d]' % d\n",
    "        assert x_val.ndim == 2, 'Expected validation data to be of rank 2'\n",
    "        assert x_val.shape[1] == d, 'Expected x_val to be [-1, %d]' % d\n",
    "        assert y_train.ndim == 2, 'Expected training labels to be of rank 2'\n",
    "        assert y_train.shape[1] == L, 'Expected y_train to be [-1, %d]' % L\n",
    "        assert y_val.ndim == 2, 'Expected validation labels to be of rank 2'\n",
    "        assert y_val.shape[1] == L, 'Expected y_val to be [-1, %d]' % L\n",
    "\n",
    "        trainNumBatches = int(np.ceil(len(x_train) / batchSize))\n",
    "        valNumBatches = int(np.ceil(len(x_val) / batchSize))\n",
    "        x_train_batches = np.array_split(x_train, trainNumBatches)\n",
    "        y_train_batches = np.array_split(y_train, trainNumBatches)\n",
    "        x_val_batches = np.array_split(x_val, valNumBatches)\n",
    "        y_val_batches = np.array_split(y_val, valNumBatches)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(x_train_batches)):\n",
    "                x_batch, y_batch = x_train_batches[i], y_train_batches[i]\n",
    "                x_batch, y_batch = torch.Tensor(x_batch), torch.Tensor(y_batch)\n",
    "                x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.protoNNObj.forward(x_batch)\n",
    "                loss = self.loss(logits, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                _, predictions = torch.max(logits, dim=1)\n",
    "                _, target = torch.max(y_batch, dim=1)\n",
    "                acc, _ = self.accuracy(predictions, target)\n",
    "                if i % printStep == 0:\n",
    "                    print(\"Epoch %d batch %d loss %f acc %f\" % (epoch, i, loss,acc))\n",
    "            # Perform IHT Here.\n",
    "            if self.sparseTraining:\n",
    "                self.hardThreshold()\n",
    "            # Perform validation set evaluation\n",
    "            if (epoch + 1) % valStep == 0:\n",
    "                numCorrect = 0\n",
    "                for i in range(len(x_val_batches)):\n",
    "                    x_batch, y_batch = x_val_batches[i], y_val_batches[i]\n",
    "                    x_batch, y_batch = torch.Tensor(x_batch), torch.Tensor(y_batch)\n",
    "                    x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
    "                    logits = self.protoNNObj.forward(x_batch)\n",
    "                    _, predictions = torch.max(logits, dim=1)\n",
    "                    _, target = torch.max(y_batch, dim=1)\n",
    "                    _, count = self.accuracy(predictions, target)\n",
    "                    numCorrect += count\n",
    "                print(\"Validation accuracy: %f\" % (numCorrect / len(x_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ebfdf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Dimension:  45\n",
      "Num classes:  3\n"
     ]
    }
   ],
   "source": [
    "#Extract the features and the predictors\n",
    "time_data = pd.read_csv(\"/Users/vanshika/Downloads/dataset_fog_release/dataset_fog_release/dataset/feature.csv\")\n",
    "target = time_data['0'] #action 0, 1, 2\n",
    "time_data = time_data.drop(['0'], axis = 1)\n",
    "\n",
    "scaler = MinMaxScaler((-1, 1)) #scaling\n",
    "X = scaler.fit_transform(time_data)\n",
    "Y = target\n",
    "\n",
    "#Split training data \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .25, random_state = 7)\n",
    "\n",
    "# windowLen = '4'\n",
    "# out = preprocessData(DATA_DIR,windowLen)\n",
    "dataDimension = X.shape[1]\n",
    "numClasses = len(np.unique(Y))\n",
    "\n",
    "print(\"Feature Dimension: \", dataDimension)\n",
    "print(\"Num classes: \", numClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "453dafcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTION_DIM = 5 #d^\n",
    "NUM_PROTOTYPES = 40 #m\n",
    "REG_W = 0.000005\n",
    "REG_B = 0.0\n",
    "REG_Z = 0.00005\n",
    "SPAR_W = 1.0\n",
    "SPAR_B = 0.8\n",
    "SPAR_Z = 0.8\n",
    "LEARNING_RATE = 0.05\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf41ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "\n",
    "def medianHeuristic(data, projectionDimension, numPrototypes, W_init=None):\n",
    "    '''\n",
    "    This method can be used to estimate gamma for ProtoNN. An approximation to\n",
    "    median heuristic is used here.\n",
    "    1. First the data is collapsed into the projectionDimension by W_init. If\n",
    "    W_init is not provided, it is initialized from a random normal(0, 1). Hence\n",
    "    data normalization is essential.\n",
    "    2. Prototype are computed by running a  k-means clustering on the projected\n",
    "    data.\n",
    "    3. The median distance is then estimated by calculating median distance\n",
    "    between prototypes and projected data points.\n",
    "\n",
    "    data needs to be [-1, numFeats]\n",
    "    If using this method to initialize gamma, please use the W and B as well.\n",
    "\n",
    "    TODO: Return estimate of Z (prototype labels) based on cluster centroids\n",
    "    andand labels\n",
    "\n",
    "    TODO: Clustering fails due to singularity error if projecting upwards\n",
    "\n",
    "    W [dxd_cap]\n",
    "    B [d_cap, m]\n",
    "    returns gamma, W, B\n",
    "    '''\n",
    "    assert data.ndim == 2\n",
    "    X = data\n",
    "    featDim = data.shape[1]\n",
    "    if projectionDimension > featDim:\n",
    "        print(\"Warning: Projection dimension > feature dimension. Gamma\")\n",
    "        print(\"\\t estimation due to median heuristic could fail.\")\n",
    "        print(\"\\tTo retain the projection dataDimension, provide\")\n",
    "        print(\"\\ta value for gamma.\")\n",
    "\n",
    "    if W_init is None:\n",
    "        W_init = np.random.normal(size=[featDim, projectionDimension])\n",
    "    W = W_init\n",
    "    XW = np.matmul(X, W)\n",
    "    assert XW.shape[1] == projectionDimension\n",
    "    assert XW.shape[0] == len(X)\n",
    "    # Requires [N x d_cap] data matrix of N observations of d_cap-dimension and\n",
    "    # the number of centroids m. Returns, [n x d_cap] centroids and\n",
    "    # elementwise center information.\n",
    "    B, centers = scipy.cluster.vq.kmeans2(XW, numPrototypes)\n",
    "    # Requires two matrices. Number of observations x dimension of observation\n",
    "    # space. Distances[i,j] is the distance between XW[i] and B[j]\n",
    "    distances = scipy.spatial.distance.cdist(XW, B, metric='euclidean')\n",
    "    distances = np.reshape(distances, [-1])\n",
    "    gamma = np.median(distances)\n",
    "    gamma = 1 / (2.5 * gamma)\n",
    "    return gamma.astype('float32'), W.astype('float32'), B.T.astype('float32')\n",
    "\n",
    "#helper methods\n",
    "def getGamma(gammaInit, projectionDim, dataDim, numPrototypes, x_train):\n",
    "    if gammaInit is None:\n",
    "        print(\"Using median heuristic to estimate gamma.\")\n",
    "        gamma, W, B = medianHeuristic(x_train, projectionDim,\n",
    "                                            numPrototypes)\n",
    "        print(\"Gamma estimate is: %f\" % gamma)\n",
    "        return W, B, gamma\n",
    "    return None, None, gammaInit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27293cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using median heuristic to estimate gamma.\n",
      "Gamma estimate is: 0.218225\n"
     ]
    }
   ],
   "source": [
    "W, B, gamma = getGamma(GAMMA, PROJECTION_DIM, dataDimension,\n",
    "                       NUM_PROTOTYPES, X_train) #x_train for small gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "869c383c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'lossType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13068/3983103515.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m                   gamma, W=W, B=B)\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m trainer = ProtoNNTrainer(protoNN, REG_W, REG_B, REG_Z,\n\u001b[0m\u001b[0;32m     14\u001b[0m                          \u001b[0mSPAR_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSPAR_B\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSPAR_Z\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                          LEARNING_RATE, X_, Y_, lossType='xentropy')\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got multiple values for argument 'lossType'"
     ]
    }
   ],
   "source": [
    "# Setup input and train protoNN\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "X_ = tf.compat.v1.placeholder(tf.float32, [None, dataDimension], name='X_')\n",
    "Y_ = tf.compat.v1.placeholder(tf.float32, [None, numClasses], name='Y_')\n",
    "\n",
    "#W=tf.convert_to_tensor(W)\n",
    "protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n",
    "                  NUM_PROTOTYPES, numClasses,\n",
    "                  gamma, W=W, B=B)\n",
    "\n",
    "trainer = ProtoNNTrainer(protoNN, REG_W, REG_B, REG_Z,\n",
    "                         SPAR_W, SPAR_B, SPAR_Z,\n",
    "                         LEARNING_RATE, X_, Y_, lossType='xentropy')\n",
    "sess = tf.Session()\n",
    "\n",
    "trainer.train(BATCH_SIZE, NUM_EPOCHS, sess, X_train, X_test, Y_train, Y_test,\n",
    "              printStep=600, valStep=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9165dae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
